{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://fbref.com/en/comps/9/2017-2018/defense/2017-2018-Premier-League-Stats'\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "with open(\"defender-old-data.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "# This is extracting the raw html from the page into a local HTML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 935 has been deleted from defender-old-data.html.\n"
     ]
    }
   ],
   "source": [
    "# Path to your saved HTML file\n",
    "html_file = \"defender-old-data.html\"\n",
    "\n",
    "# Read the HTML file\n",
    "with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Specify the line you want to delete (e.g., line number 10)\n",
    "line_to_delete = 934\n",
    "\n",
    "# Remove the specified line\n",
    "if 0 <= line_to_delete < len(lines):\n",
    "    del lines[line_to_delete]\n",
    "\n",
    "# Write the modified content back to the HTML file\n",
    "with open(html_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(lines)\n",
    "\n",
    "print(f\"Line {line_to_delete + 1} has been deleted from {html_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found <table id='stats_defense'>.\n",
      "Preview of <table> content:\n",
      " <table class=\"min_width sortable stats_table min_width shade_zero\" data-cols-to-freeze=\",2\" data-non-qual=\"1\" data-qual-label=\"Hide non-qualifiers for rate stats\" data-qual-text=\"\" id=\"stats_defense\">\n",
      " <caption>\n",
      "  Player Defensive Actions\n",
      "  <span style=\"color: #666; font-size:smaller\">\n",
      "   2017-2018 Premier League\n",
      "  </span>\n",
      "  Table\n",
      " </caption>\n",
      " <colgroup>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "  <col/>\n",
      "Player stats saved to '17_18_defending.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/fbwfkpy50vs_7m6dlkzl468h0000gn/T/ipykernel_83787/861000000.py:21: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your saved HTML file\n",
    "html_file = \"defender-old-data.html\"\n",
    "\n",
    "# Load the HTML file with BeautifulSoup\n",
    "with open(html_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    soup = BeautifulSoup(file, \"lxml\")  # You can also try 'lxml' if needed\n",
    "\n",
    "# Locate the <table> with id 'stats_defense'\n",
    "table = soup.find(\"table\", {\"id\": \"stats_defense\"})\n",
    "\n",
    "if table:\n",
    "    print(\"Found <table id='stats_defense'>.\")\n",
    "    # Print the first 500 characters of the table for inspection\n",
    "    print(\"Preview of <table> content:\\n\", table.prettify()[:500])\n",
    "\n",
    "    # Parse the table using pandas\n",
    "    try:\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(\"17_18_defending.csv\", index=False)\n",
    "        print(\"Player stats saved to '17_18_defending.csv'\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing the table: {e}\")\n",
    "else:\n",
    "    print(\"No <table> with id 'stats_defense' found.\")\n",
    "\n",
    "# This code was obtained from ChatGPT and it is used to extract tables from HTML files using BeautifulSoup and pandas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Of Previous Season Data Extraction\n",
    "\n",
    "Overall this wasn't too bad to achieve as it did follow the same process as acquiring the current season data. It took a little bit of time to find the data on the FBREF site and then the older season data follows a slightly different structure than the current season one. I assume this is because it is being updated on a weekly or bi-weekly basis. Once I figured out where the difference was it was striaghtforward getting the data out as it is structured the same.\n",
    "\n",
    "This will be very helpful as the more data that I can acquire the better as it will help to create a better model for making the predictions for the site in the future. I can now go back a few seasons and get a few thousands rows of data rather than just the 500 that exist for the current season."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
